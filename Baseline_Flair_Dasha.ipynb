{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Baseline-Flair.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EB9Lki-FtkGb"
      },
      "source": [
        "Устанавливаем allennlp, чтобы выгрузить оттуда эмбеддинги ELMo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtM7-LAoeMTO",
        "outputId": "6fb937e7-8054-47d4-f8e0-aee292b0ad69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install flair"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: flair in /usr/local/lib/python3.6/dist-packages (0.7)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from flair) (1.7.0+cu101)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.6/dist-packages (from flair) (1.0.8)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.6/dist-packages (from flair) (0.22.2.post1)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from flair) (2.8.1)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from flair) (3.2.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from flair) (2019.12.20)\n",
            "Requirement already satisfied: mpld3==0.3 in /usr/local/lib/python3.6/dist-packages (from flair) (0.3)\n",
            "Requirement already satisfied: bpemb>=0.3.2 in /usr/local/lib/python3.6/dist-packages (from flair) (0.3.2)\n",
            "Requirement already satisfied: sqlitedict>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from flair) (1.7.0)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.6/dist-packages (from flair) (5.8)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.6/dist-packages (from flair) (3.6.4)\n",
            "Requirement already satisfied: sentencepiece<=0.1.91 in /usr/local/lib/python3.6/dist-packages (from flair) (0.1.91)\n",
            "Requirement already satisfied: gensim<=3.8.3,>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from flair) (3.6.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from flair) (0.8.7)\n",
            "Requirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.6/dist-packages (from flair) (4.41.1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (from flair) (4.2.6)\n",
            "Requirement already satisfied: segtok>=1.5.7 in /usr/local/lib/python3.6/dist-packages (from flair) (1.5.10)\n",
            "Requirement already satisfied: hyperopt>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from flair) (0.1.2)\n",
            "Requirement already satisfied: konoha<5.0.0,>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from flair) (4.6.2)\n",
            "Requirement already satisfied: deprecated>=1.2.4 in /usr/local/lib/python3.6/dist-packages (from flair) (1.2.11)\n",
            "Requirement already satisfied: janome in /usr/local/lib/python3.6/dist-packages (from flair) (0.4.1)\n",
            "Collecting transformers<=3.5.1,>=3.5.0\n",
            "  Using cached https://files.pythonhosted.org/packages/3a/83/e74092e7f24a08d751aa59b37a9fc572b2e4af3918cb66f7766c3affb1b4/transformers-3.5.1-py3-none-any.whl\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.1.0->flair) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.1.0->flair) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch>=1.1.0->flair) (1.19.5)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.1.0->flair) (0.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from langdetect->flair) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->flair) (1.0.0)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->flair) (1.4.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (1.3.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from bpemb>=0.3.2->flair) (2.23.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy->flair) (0.2.5)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim<=3.8.3,>=3.4.0->flair) (4.1.2)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (3.11.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (2.5)\n",
            "Processing /root/.cache/pip/wheels/6f/1b/ec/6c71a1eb823df7f850d956b2d8c50a6d49c191e1063d73b9be/overrides-3.0.0-cp36-none-any.whl\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.6/dist-packages (from deprecated>=1.2.4->flair) (1.12.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers<=3.5.1,>=3.5.0->flair) (20.9)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers<=3.5.1,>=3.5.0->flair) (3.12.4)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers<=3.5.1,>=3.5.0->flair) (0.0.43)\n",
            "Collecting tokenizers==0.9.3\n",
            "  Using cached https://files.pythonhosted.org/packages/4c/34/b39eb9994bc3c999270b69c9eea40ecc6f0e97991dba28282b9fd32d44ee/tokenizers-0.9.3-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers<=3.5.1,>=3.5.0->flair) (3.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb>=0.3.2->flair) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb>=0.3.2->flair) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb>=0.3.2->flair) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb>=0.3.2->flair) (2.10)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->hyperopt>=0.1.1->flair) (4.4.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers<=3.5.1,>=3.5.0->flair) (53.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers<=3.5.1,>=3.5.0->flair) (7.1.2)\n",
            "\u001b[31mERROR: allennlp 2.0.1 has requirement overrides==3.1.0, but you'll have overrides 3.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: allennlp 2.0.1 has requirement transformers<4.3,>=4.1, but you'll have transformers 3.5.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tokenizers, transformers, overrides\n",
            "  Found existing installation: tokenizers 0.9.4\n",
            "    Uninstalling tokenizers-0.9.4:\n",
            "      Successfully uninstalled tokenizers-0.9.4\n",
            "  Found existing installation: transformers 4.2.2\n",
            "    Uninstalling transformers-4.2.2:\n",
            "      Successfully uninstalled transformers-4.2.2\n",
            "  Found existing installation: overrides 3.1.0\n",
            "    Uninstalling overrides-3.1.0:\n",
            "      Successfully uninstalled overrides-3.1.0\n",
            "Successfully installed overrides-3.0.0 tokenizers-0.9.3 transformers-3.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9P3yj0ZTeMTg",
        "outputId": "474a69f4-6fdd-44a1-9f7e-fd813d8a71cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Uncomment if you need to download data\n",
        "!wget https://raw.githubusercontent.com/Rexhaif/ner-dialogues-hackathon/master/data/train.conll\n",
        "!wget https://raw.githubusercontent.com/Rexhaif/ner-dialogues-hackathon/master/data/dev.conll\n",
        "!wget https://raw.githubusercontent.com/Rexhaif/ner-dialogues-hackathon/master/data/test.conll"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-09 17:29:59--  https://raw.githubusercontent.com/Rexhaif/ner-dialogues-hackathon/master/data/train.conll\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 125059 (122K) [text/plain]\n",
            "Saving to: ‘train.conll.3’\n",
            "\n",
            "train.conll.3       100%[===================>] 122.13K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2021-02-09 17:29:59 (4.66 MB/s) - ‘train.conll.3’ saved [125059/125059]\n",
            "\n",
            "--2021-02-09 17:29:59--  https://raw.githubusercontent.com/Rexhaif/ner-dialogues-hackathon/master/data/dev.conll\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14109 (14K) [text/plain]\n",
            "Saving to: ‘dev.conll.3’\n",
            "\n",
            "dev.conll.3         100%[===================>]  13.78K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-02-09 17:30:00 (92.9 MB/s) - ‘dev.conll.3’ saved [14109/14109]\n",
            "\n",
            "--2021-02-09 17:30:00--  https://raw.githubusercontent.com/Rexhaif/ner-dialogues-hackathon/master/data/test.conll\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 34661 (34K) [text/plain]\n",
            "Saving to: ‘test.conll.3’\n",
            "\n",
            "test.conll.3        100%[===================>]  33.85K  --.-KB/s    in 0.004s  \n",
            "\n",
            "2021-02-09 17:30:00 (8.53 MB/s) - ‘test.conll.3’ saved [34661/34661]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPTz4z_QeMTn"
      },
      "source": [
        "from flair.data import Corpus\n",
        "from flair.datasets import ColumnCorpus\n",
        "from flair.embeddings import TokenEmbeddings, WordEmbeddings, StackedEmbeddings, CharacterEmbeddings, FlairEmbeddings, BytePairEmbeddings, TransformerWordEmbeddings"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dlNwO-leMTo"
      },
      "source": [
        "import flair\n",
        "import torch\n",
        "# uncomment for cpu-only training\n",
        "#flair.device = torch.device('cpu')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyFZPDs9eMTp",
        "outputId": "87a7df39-70bf-4b6b-e0e0-30bd988e8b44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# 1. get the corpus\n",
        "corpus: Corpus = ColumnCorpus(\n",
        "    \"./\", column_format={0:'text', 1:'ner'},\n",
        "    train_file='train.conll', dev_file='dev.conll', test_file='test.conll',\n",
        ")\n",
        "print(corpus)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-02-09 17:01:02,680 Reading data from .\n",
            "2021-02-09 17:01:02,683 Train: train.conll\n",
            "2021-02-09 17:01:02,685 Dev: dev.conll\n",
            "2021-02-09 17:01:02,687 Test: test.conll\n",
            "Corpus: 1966 train + 219 dev + 547 test sentences\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcKG_yaMeMTq",
        "outputId": "b9114226-31a9-4fa2-a59a-7b665e60f9ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# 2. what tag do we want to predict?\n",
        "tag_type = 'ner'\n",
        "\n",
        "# 3. make the tag dictionary from the corpus\n",
        "tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)\n",
        "print(tag_dictionary)\n",
        "\n",
        "# 4. initialize embeddings\n",
        "#embedding_types = [\n",
        " #   BytePairEmbeddings('multi')\n",
        "#]\n",
        "\n",
        "#embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dictionary with 14 tags: <unk>, O, B-SINGER, I-SINGER, B-SONG, B-FILM, I-SONG, I-FILM, B-COMPOSER, I-COMPOSER, B-BOOK, I-BOOK, <START>, <STOP>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Rqg0sJiqJF1"
      },
      "source": [
        "Автоматически подбираем гиперпарметры:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_l24LYUjqOOe"
      },
      "source": [
        "from hyperopt import hp\r\n",
        "from flair.hyperparameter.param_selection import SearchSpace, Parameter"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0ZnwAeRqQMT",
        "outputId": "393fe6e3-7c5b-42de-b3eb-8266b7ef17ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "search_space = SearchSpace()\r\n",
        "search_space.add(Parameter.EMBEDDINGS, hp.choice, options=[\r\n",
        "    StackedEmbeddings(embeddings=[BytePairEmbeddings('multi'), FlairEmbeddings('news-forward'), TransformerWordEmbeddings('bert-base-multilingual-cased'),TransformerWordEmbeddings('roberta-base')])\r\n",
        "])\r\n",
        "search_space.add(Parameter.HIDDEN_SIZE, hp.choice, options=[64, 128, 256])\r\n",
        "search_space.add(Parameter.LEARNING_RATE, hp.choice, options=[0.05, 0.1, 0.15, 0.2])\r\n",
        "search_space.add(Parameter.MINI_BATCH_SIZE, hp.choice, options=[8, 16, 32])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting dim=300 for multilingual BPEmb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A559cBO8rwze"
      },
      "source": [
        "Обучение:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLp09ADBr1g6",
        "outputId": "bafd7071-e457-4cc3-fd55-51f511d23c6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from flair.hyperparameter.param_selection import SequenceTaggerParamSelector, OptimizationValue\r\n",
        "\r\n",
        "\r\n",
        "optimizer = SequenceTaggerParamSelector(\r\n",
        "        corpus, \"ner\", './results', max_epochs=2\r\n",
        "    )\r\n",
        "optimizer.optimize(search_space, max_evals=3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/50 [00:00<?, ?it/s, best loss: ?]2021-02-09 17:23:31,791 ----------------------------------------------------------------------------------------------------\n",
            "2021-02-09 17:23:31,792 Evaluation run: 1\n",
            "2021-02-09 17:23:31,795 Evaluating parameter combination:\n",
            "2021-02-09 17:23:31,801 \tembeddings: StackedEmbeddings [0-bpe-multi-100000-50,1-/root/.flair/embeddings/news-forward-0.4.1.pt,TransformerWordEmbeddings(\n",
            "  (model): BertModel(\n",
            "    (embeddings): BertEmbeddings(\n",
            "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
            "      (position_embeddings): Embedding(512, 768)\n",
            "      (token_type_embeddings): Embedding(2, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): BertEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (2): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (3): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (4): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (5): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (6): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (7): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (8): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (9): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (10): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (11): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): BertPooler(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (activation): Tanh()\n",
            "    )\n",
            "  )\n",
            "),TransformerWordEmbeddings(\n",
            "  (model): RobertaModel(\n",
            "    (embeddings): RobertaEmbeddings(\n",
            "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
            "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
            "      (token_type_embeddings): Embedding(1, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): RobertaEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (2): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (3): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (4): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (5): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (6): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (7): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (8): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (9): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (10): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (11): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): RobertaPooler(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (activation): Tanh()\n",
            "    )\n",
            "  )\n",
            ")]\n",
            "2021-02-09 17:23:31,803 \thidden_size: 128\n",
            "2021-02-09 17:23:31,804 \tlearning_rate: 0.05\n",
            "2021-02-09 17:23:31,805 \tmini_batch_size: 16\n",
            "2021-02-09 17:23:31,806 ----------------------------------------------------------------------------------------------------\n",
            "2021-02-09 17:23:31,820 ----------------------------------------------------------------------------------------------------\n",
            "2021-02-09 17:23:31,822 Training run: 1\n",
            "2021-02-09 17:23:32,585 ----------------------------------------------------------------------------------------------------\n",
            "2021-02-09 17:23:32,592 Model: \"SequenceTagger(\n",
            "  (embeddings): StackedEmbeddings(\n",
            "    (list_embedding_0): BytePairEmbeddings(model=0-bpe-multi-100000-50)\n",
            "    (list_embedding_1): FlairEmbeddings(\n",
            "      (lm): LanguageModel(\n",
            "        (drop): Dropout(p=0.05, inplace=False)\n",
            "        (encoder): Embedding(300, 100)\n",
            "        (rnn): LSTM(100, 2048)\n",
            "        (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
            "      )\n",
            "    )\n",
            "    (list_embedding_2): TransformerWordEmbeddings(\n",
            "      (model): BertModel(\n",
            "        (embeddings): BertEmbeddings(\n",
            "          (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
            "          (position_embeddings): Embedding(512, 768)\n",
            "          (token_type_embeddings): Embedding(2, 768)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (encoder): BertEncoder(\n",
            "          (layer): ModuleList(\n",
            "            (0): BertLayer(\n",
            "              (attention): BertAttention(\n",
            "                (self): BertSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): BertSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): BertIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): BertOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (1): BertLayer(\n",
            "              (attention): BertAttention(\n",
            "                (self): BertSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): BertSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): BertIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): BertOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (2): BertLayer(\n",
            "              (attention): BertAttention(\n",
            "                (self): BertSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): BertSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): BertIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): BertOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (3): BertLayer(\n",
            "              (attention): BertAttention(\n",
            "                (self): BertSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): BertSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): BertIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): BertOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (4): BertLayer(\n",
            "              (attention): BertAttention(\n",
            "                (self): BertSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): BertSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): BertIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): BertOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (5): BertLayer(\n",
            "              (attention): BertAttention(\n",
            "                (self): BertSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): BertSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): BertIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): BertOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (6): BertLayer(\n",
            "              (attention): BertAttention(\n",
            "                (self): BertSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): BertSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): BertIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): BertOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (7): BertLayer(\n",
            "              (attention): BertAttention(\n",
            "                (self): BertSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): BertSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): BertIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): BertOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (8): BertLayer(\n",
            "              (attention): BertAttention(\n",
            "                (self): BertSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): BertSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): BertIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): BertOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (9): BertLayer(\n",
            "              (attention): BertAttention(\n",
            "                (self): BertSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): BertSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): BertIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): BertOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (10): BertLayer(\n",
            "              (attention): BertAttention(\n",
            "                (self): BertSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): BertSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): BertIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): BertOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (11): BertLayer(\n",
            "              (attention): BertAttention(\n",
            "                (self): BertSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): BertSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): BertIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): BertOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (pooler): BertPooler(\n",
            "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (activation): Tanh()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (list_embedding_3): TransformerWordEmbeddings(\n",
            "      (model): RobertaModel(\n",
            "        (embeddings): RobertaEmbeddings(\n",
            "          (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
            "          (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
            "          (token_type_embeddings): Embedding(1, 768)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (encoder): RobertaEncoder(\n",
            "          (layer): ModuleList(\n",
            "            (0): RobertaLayer(\n",
            "              (attention): RobertaAttention(\n",
            "                (self): RobertaSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): RobertaSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): RobertaIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): RobertaOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (1): RobertaLayer(\n",
            "              (attention): RobertaAttention(\n",
            "                (self): RobertaSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): RobertaSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): RobertaIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): RobertaOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (2): RobertaLayer(\n",
            "              (attention): RobertaAttention(\n",
            "                (self): RobertaSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): RobertaSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): RobertaIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): RobertaOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (3): RobertaLayer(\n",
            "              (attention): RobertaAttention(\n",
            "                (self): RobertaSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): RobertaSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): RobertaIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): RobertaOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (4): RobertaLayer(\n",
            "              (attention): RobertaAttention(\n",
            "                (self): RobertaSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): RobertaSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): RobertaIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): RobertaOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (5): RobertaLayer(\n",
            "              (attention): RobertaAttention(\n",
            "                (self): RobertaSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): RobertaSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): RobertaIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): RobertaOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (6): RobertaLayer(\n",
            "              (attention): RobertaAttention(\n",
            "                (self): RobertaSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): RobertaSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): RobertaIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): RobertaOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (7): RobertaLayer(\n",
            "              (attention): RobertaAttention(\n",
            "                (self): RobertaSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): RobertaSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): RobertaIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): RobertaOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (8): RobertaLayer(\n",
            "              (attention): RobertaAttention(\n",
            "                (self): RobertaSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): RobertaSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): RobertaIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): RobertaOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (9): RobertaLayer(\n",
            "              (attention): RobertaAttention(\n",
            "                (self): RobertaSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): RobertaSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): RobertaIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): RobertaOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (10): RobertaLayer(\n",
            "              (attention): RobertaAttention(\n",
            "                (self): RobertaSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): RobertaSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): RobertaIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): RobertaOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (11): RobertaLayer(\n",
            "              (attention): RobertaAttention(\n",
            "                (self): RobertaSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): RobertaSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): RobertaIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): RobertaOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (pooler): RobertaPooler(\n",
            "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (activation): Tanh()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (word_dropout): WordDropout(p=0.05)\n",
            "  (locked_dropout): LockedDropout(p=0.5)\n",
            "  (embedding2nn): Linear(in_features=8792, out_features=8792, bias=True)\n",
            "  (rnn): LSTM(8792, 128, batch_first=True, bidirectional=True)\n",
            "  (linear): Linear(in_features=256, out_features=14, bias=True)\n",
            "  (beta): 1.0\n",
            "  (weights): None\n",
            "  (weight_tensor) None\n",
            ")\"\n",
            "2021-02-09 17:23:32,594 ----------------------------------------------------------------------------------------------------\n",
            "2021-02-09 17:23:32,595 Corpus: \"Corpus: 1966 train + 219 dev + 547 test sentences\"\n",
            "2021-02-09 17:23:32,597 ----------------------------------------------------------------------------------------------------\n",
            "2021-02-09 17:23:32,598 Parameters:\n",
            "2021-02-09 17:23:32,599  - learning_rate: \"0.05\"\n",
            "2021-02-09 17:23:32,600  - mini_batch_size: \"16\"\n",
            "2021-02-09 17:23:32,601  - patience: \"3\"\n",
            "2021-02-09 17:23:32,602  - anneal_factor: \"0.5\"\n",
            "2021-02-09 17:23:32,603  - max_epochs: \"3\"\n",
            "2021-02-09 17:23:32,604  - shuffle: \"True\"\n",
            "2021-02-09 17:23:32,605  - train_with_dev: \"False\"\n",
            "2021-02-09 17:23:32,606  - batch_growth_annealing: \"False\"\n",
            "2021-02-09 17:23:32,607 ----------------------------------------------------------------------------------------------------\n",
            "2021-02-09 17:23:32,608 Model training base path: \"results\"\n",
            "2021-02-09 17:23:32,608 ----------------------------------------------------------------------------------------------------\n",
            "2021-02-09 17:23:32,609 Device: cpu\n",
            "2021-02-09 17:23:32,610 ----------------------------------------------------------------------------------------------------\n",
            "2021-02-09 17:23:32,611 Embeddings storage mode: cpu\n",
            "2021-02-09 17:23:32,620 ----------------------------------------------------------------------------------------------------\n",
            "2021-02-09 17:24:46,561 epoch 1 - iter 12/123 - loss 9.63844268 - samples/sec: 2.60 - lr: 0.050000\n",
            "2021-02-09 17:25:58,414 epoch 1 - iter 24/123 - loss 8.72515523 - samples/sec: 2.67 - lr: 0.050000\n",
            "2021-02-09 17:26:13,111 ----------------------------------------------------------------------------------------------------\n",
            "2021-02-09 17:26:13,113 Exiting from training early.\n",
            "2021-02-09 17:26:13,115 ----------------------------------------------------------------------------------------------------\n",
            "2021-02-09 17:26:13,117 Testing using best model ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGbm-dg-eMTu",
        "outputId": "966db7dd-13f4-4d25-aed3-8730dc28f2b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# 5. initialize sequence tagger\n",
        "# from flair.models import SequenceTagger\n",
        "\n",
        "# tagger: SequenceTagger = SequenceTagger(hidden_size=256,\n",
        "#                                         embeddings=embeddings,\n",
        "#                                         tag_dictionary=tag_dictionary,\n",
        "#                                         tag_type=tag_type,\n",
        "#                                         use_crf=True)\n",
        "\n",
        "# # 6. initialize trainer\n",
        "# from flair.trainers import ModelTrainer\n",
        "\n",
        "# trainer: ModelTrainer = ModelTrainer(tagger, corpus)\n",
        "\n",
        "# # 7. start training\n",
        "# history = trainer.train(\n",
        "#     './models/baseline-charembeddings',\n",
        "#     learning_rate=0.1,\n",
        "#     mini_batch_size=32,\n",
        "#     max_epochs=20\n",
        "# )"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-02-09 16:20:59,587 ----------------------------------------------------------------------------------------------------\n",
            "2021-02-09 16:20:59,588 Model: \"SequenceTagger(\n",
            "  (embeddings): StackedEmbeddings(\n",
            "    (list_embedding_0): BytePairEmbeddings(model=0-bpe-multi-100000-50)\n",
            "  )\n",
            "  (word_dropout): WordDropout(p=0.05)\n",
            "  (locked_dropout): LockedDropout(p=0.5)\n",
            "  (embedding2nn): Linear(in_features=600, out_features=600, bias=True)\n",
            "  (rnn): LSTM(600, 256, batch_first=True, bidirectional=True)\n",
            "  (linear): Linear(in_features=512, out_features=14, bias=True)\n",
            "  (beta): 1.0\n",
            "  (weights): None\n",
            "  (weight_tensor) None\n",
            ")\"\n",
            "2021-02-09 16:20:59,589 ----------------------------------------------------------------------------------------------------\n",
            "2021-02-09 16:20:59,590 Corpus: \"Corpus: 1966 train + 219 dev + 547 test sentences\"\n",
            "2021-02-09 16:20:59,591 ----------------------------------------------------------------------------------------------------\n",
            "2021-02-09 16:20:59,592 Parameters:\n",
            "2021-02-09 16:20:59,594  - learning_rate: \"0.1\"\n",
            "2021-02-09 16:20:59,595  - mini_batch_size: \"32\"\n",
            "2021-02-09 16:20:59,596  - patience: \"3\"\n",
            "2021-02-09 16:20:59,605  - anneal_factor: \"0.5\"\n",
            "2021-02-09 16:20:59,609  - max_epochs: \"20\"\n",
            "2021-02-09 16:20:59,614  - shuffle: \"True\"\n",
            "2021-02-09 16:20:59,619  - train_with_dev: \"False\"\n",
            "2021-02-09 16:20:59,620  - batch_growth_annealing: \"False\"\n",
            "2021-02-09 16:20:59,621 ----------------------------------------------------------------------------------------------------\n",
            "2021-02-09 16:20:59,625 Model training base path: \"models/baseline-charembeddings\"\n",
            "2021-02-09 16:20:59,627 ----------------------------------------------------------------------------------------------------\n",
            "2021-02-09 16:20:59,628 Device: cpu\n",
            "2021-02-09 16:20:59,629 ----------------------------------------------------------------------------------------------------\n",
            "2021-02-09 16:20:59,632 Embeddings storage mode: cpu\n",
            "2021-02-09 16:20:59,634 ----------------------------------------------------------------------------------------------------\n",
            "2021-02-09 16:21:00,198 epoch 1 - iter 6/62 - loss 9.56844012 - samples/sec: 342.46 - lr: 0.100000\n",
            "2021-02-09 16:21:00,755 epoch 1 - iter 12/62 - loss 8.45471791 - samples/sec: 345.65 - lr: 0.100000\n",
            "2021-02-09 16:21:01,312 epoch 1 - iter 18/62 - loss 7.87777784 - samples/sec: 346.22 - lr: 0.100000\n",
            "2021-02-09 16:21:01,868 epoch 1 - iter 24/62 - loss 7.47612254 - samples/sec: 346.17 - lr: 0.100000\n",
            "2021-02-09 16:21:02,417 epoch 1 - iter 30/62 - loss 7.20383347 - samples/sec: 350.59 - lr: 0.100000\n",
            "2021-02-09 16:21:02,917 epoch 1 - iter 36/62 - loss 6.80251699 - samples/sec: 385.70 - lr: 0.100000\n",
            "2021-02-09 16:21:03,488 epoch 1 - iter 42/62 - loss 6.55235479 - samples/sec: 337.15 - lr: 0.100000\n",
            "2021-02-09 16:21:04,023 epoch 1 - iter 48/62 - loss 6.28822039 - samples/sec: 359.31 - lr: 0.100000\n",
            "2021-02-09 16:21:04,581 epoch 1 - iter 54/62 - loss 6.12408224 - samples/sec: 345.18 - lr: 0.100000\n",
            "2021-02-09 16:21:05,151 epoch 1 - iter 60/62 - loss 5.95886458 - samples/sec: 337.58 - lr: 0.100000\n",
            "2021-02-09 16:21:05,310 ----------------------------------------------------------------------------------------------------\n",
            "2021-02-09 16:21:05,311 EPOCH 1 done: loss 5.8956 - lr 0.1000000\n",
            "2021-02-09 16:21:05,559 DEV : loss 4.061650276184082 - score 0.3559\n",
            "2021-02-09 16:21:05,567 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2021-02-09 16:21:06,838 ----------------------------------------------------------------------------------------------------\n",
            "2021-02-09 16:21:07,370 epoch 2 - iter 6/62 - loss 4.11523934 - samples/sec: 365.84 - lr: 0.100000\n",
            "2021-02-09 16:21:07,921 epoch 2 - iter 12/62 - loss 4.19823724 - samples/sec: 349.32 - lr: 0.100000\n",
            "2021-02-09 16:21:08,431 epoch 2 - iter 18/62 - loss 4.08964590 - samples/sec: 378.08 - lr: 0.100000\n",
            "2021-02-09 16:21:08,947 epoch 2 - iter 24/62 - loss 3.93447504 - samples/sec: 372.85 - lr: 0.100000\n",
            "2021-02-09 16:21:09,467 epoch 2 - iter 30/62 - loss 3.80322065 - samples/sec: 370.52 - lr: 0.100000\n",
            "2021-02-09 16:21:09,951 epoch 2 - iter 36/62 - loss 3.73436971 - samples/sec: 397.63 - lr: 0.100000\n",
            "2021-02-09 16:21:10,477 epoch 2 - iter 42/62 - loss 3.67206219 - samples/sec: 366.55 - lr: 0.100000\n",
            "2021-02-09 16:21:11,001 epoch 2 - iter 48/62 - loss 3.62385571 - samples/sec: 367.37 - lr: 0.100000\n",
            "2021-02-09 16:21:11,510 epoch 2 - iter 54/62 - loss 3.56256784 - samples/sec: 378.61 - lr: 0.100000\n",
            "2021-02-09 16:21:12,026 epoch 2 - iter 60/62 - loss 3.50089849 - samples/sec: 373.30 - lr: 0.100000\n",
            "2021-02-09 16:21:12,173 ----------------------------------------------------------------------------------------------------\n",
            "2021-02-09 16:21:12,175 EPOCH 2 done: loss 3.4900 - lr 0.1000000\n",
            "2021-02-09 16:21:12,384 DEV : loss 3.0122568607330322 - score 0.438\n",
            "2021-02-09 16:21:12,393 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2021-02-09 16:21:13,372 ----------------------------------------------------------------------------------------------------\n",
            "2021-02-09 16:21:13,965 epoch 3 - iter 6/62 - loss 3.17747180 - samples/sec: 327.16 - lr: 0.100000\n",
            "2021-02-09 16:21:14,492 epoch 3 - iter 12/62 - loss 3.02289059 - samples/sec: 365.79 - lr: 0.100000\n",
            "2021-02-09 16:21:15,025 epoch 3 - iter 18/62 - loss 2.90123879 - samples/sec: 360.98 - lr: 0.100000\n",
            "2021-02-09 16:21:15,535 epoch 3 - iter 24/62 - loss 2.89930118 - samples/sec: 378.23 - lr: 0.100000\n",
            "2021-02-09 16:21:16,068 epoch 3 - iter 30/62 - loss 2.80230568 - samples/sec: 362.15 - lr: 0.100000\n",
            "2021-02-09 16:21:16,568 epoch 3 - iter 36/62 - loss 2.78389496 - samples/sec: 385.08 - lr: 0.100000\n",
            "2021-02-09 16:21:17,064 epoch 3 - iter 42/62 - loss 2.77059706 - samples/sec: 388.76 - lr: 0.100000\n",
            "2021-02-09 16:21:17,579 epoch 3 - iter 48/62 - loss 2.75891967 - samples/sec: 373.93 - lr: 0.100000\n",
            "2021-02-09 16:21:18,082 epoch 3 - iter 54/62 - loss 2.71159344 - samples/sec: 382.37 - lr: 0.100000\n",
            "2021-02-09 16:21:18,597 epoch 3 - iter 60/62 - loss 2.71329675 - samples/sec: 373.95 - lr: 0.100000\n",
            "2021-02-09 16:21:18,726 ----------------------------------------------------------------------------------------------------\n",
            "2021-02-09 16:21:18,727 EPOCH 3 done: loss 2.6924 - lr 0.1000000\n",
            "2021-02-09 16:21:18,949 DEV : loss 2.173452138900757 - score 0.5529\n",
            "2021-02-09 16:21:18,957 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2021-02-09 16:21:19,998 ----------------------------------------------------------------------------------------------------\n",
            "2021-02-09 16:21:20,501 epoch 4 - iter 6/62 - loss 2.44092482 - samples/sec: 387.39 - lr: 0.100000\n",
            "2021-02-09 16:21:21,013 epoch 4 - iter 12/62 - loss 2.36159458 - samples/sec: 377.45 - lr: 0.100000\n",
            "2021-02-09 16:21:21,553 epoch 4 - iter 18/62 - loss 2.40473166 - samples/sec: 357.04 - lr: 0.100000\n",
            "2021-02-09 16:21:22,073 epoch 4 - iter 24/62 - loss 2.39156477 - samples/sec: 371.02 - lr: 0.100000\n",
            "2021-02-09 16:21:22,621 epoch 4 - iter 30/62 - loss 2.41614430 - samples/sec: 354.17 - lr: 0.100000\n",
            "2021-02-09 16:21:23,141 epoch 4 - iter 36/62 - loss 2.37217628 - samples/sec: 370.37 - lr: 0.100000\n",
            "2021-02-09 16:21:23,652 epoch 4 - iter 42/62 - loss 2.31877735 - samples/sec: 376.53 - lr: 0.100000\n",
            "2021-02-09 16:21:24,188 epoch 4 - iter 48/62 - loss 2.27594916 - samples/sec: 361.49 - lr: 0.100000\n",
            "2021-02-09 16:21:24,705 epoch 4 - iter 54/62 - loss 2.25308871 - samples/sec: 374.39 - lr: 0.100000\n",
            "2021-02-09 16:21:25,233 epoch 4 - iter 60/62 - loss 2.23406289 - samples/sec: 366.84 - lr: 0.100000\n",
            "2021-02-09 16:21:25,378 ----------------------------------------------------------------------------------------------------\n",
            "2021-02-09 16:21:25,379 EPOCH 4 done: loss 2.2248 - lr 0.1000000\n",
            "2021-02-09 16:21:25,590 DEV : loss 1.8724805116653442 - score 0.5938\n",
            "2021-02-09 16:21:25,598 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2021-02-09 16:21:26,588 ----------------------------------------------------------------------------------------------------\n",
            "2021-02-09 16:21:27,185 epoch 5 - iter 6/62 - loss 2.16316350 - samples/sec: 325.16 - lr: 0.100000\n",
            "2021-02-09 16:21:27,715 epoch 5 - iter 12/62 - loss 2.06821239 - samples/sec: 365.27 - lr: 0.100000\n",
            "2021-02-09 16:21:28,220 epoch 5 - iter 18/62 - loss 2.04843699 - samples/sec: 381.72 - lr: 0.100000\n",
            "2021-02-09 16:21:28,750 epoch 5 - iter 24/62 - loss 1.98834986 - samples/sec: 363.39 - lr: 0.100000\n",
            "2021-02-09 16:21:29,245 epoch 5 - iter 30/62 - loss 1.94117916 - samples/sec: 389.38 - lr: 0.100000\n",
            "2021-02-09 16:21:29,745 epoch 5 - iter 36/62 - loss 1.95817144 - samples/sec: 385.13 - lr: 0.100000\n",
            "2021-02-09 16:21:30,235 epoch 5 - iter 42/62 - loss 1.94022634 - samples/sec: 393.06 - lr: 0.100000\n",
            "2021-02-09 16:21:30,732 epoch 5 - iter 48/62 - loss 1.92632553 - samples/sec: 387.32 - lr: 0.100000\n",
            "2021-02-09 16:21:31,269 epoch 5 - iter 54/62 - loss 1.91047429 - samples/sec: 358.62 - lr: 0.100000\n",
            "2021-02-09 16:21:31,807 epoch 5 - iter 60/62 - loss 1.90404772 - samples/sec: 358.46 - lr: 0.100000\n",
            "2021-02-09 16:21:31,942 ----------------------------------------------------------------------------------------------------\n",
            "2021-02-09 16:21:31,943 EPOCH 5 done: loss 1.8933 - lr 0.1000000\n",
            "2021-02-09 16:21:32,155 DEV : loss 1.7632169723510742 - score 0.5619\n",
            "2021-02-09 16:21:32,161 BAD EPOCHS (no improvement): 1\n",
            "2021-02-09 16:21:32,162 ----------------------------------------------------------------------------------------------------\n",
            "2021-02-09 16:21:32,671 epoch 6 - iter 6/62 - loss 1.59685286 - samples/sec: 379.25 - lr: 0.100000\n",
            "2021-02-09 16:21:33,184 epoch 6 - iter 12/62 - loss 1.65330777 - samples/sec: 376.23 - lr: 0.100000\n",
            "2021-02-09 16:21:33,754 epoch 6 - iter 18/62 - loss 1.73269467 - samples/sec: 337.87 - lr: 0.100000\n",
            "2021-02-09 16:21:34,262 epoch 6 - iter 24/62 - loss 1.78052955 - samples/sec: 379.28 - lr: 0.100000\n",
            "2021-02-09 16:21:34,765 epoch 6 - iter 30/62 - loss 1.73260874 - samples/sec: 382.27 - lr: 0.100000\n",
            "2021-02-09 16:21:35,287 epoch 6 - iter 36/62 - loss 1.71383563 - samples/sec: 369.29 - lr: 0.100000\n",
            "2021-02-09 16:21:35,785 epoch 6 - iter 42/62 - loss 1.71470481 - samples/sec: 387.01 - lr: 0.100000\n",
            "2021-02-09 16:21:36,290 epoch 6 - iter 48/62 - loss 1.69532233 - samples/sec: 381.83 - lr: 0.100000\n",
            "2021-02-09 16:21:36,820 epoch 6 - iter 54/62 - loss 1.68943385 - samples/sec: 362.91 - lr: 0.100000\n",
            "2021-02-09 16:21:37,313 epoch 6 - iter 60/62 - loss 1.68400024 - samples/sec: 391.57 - lr: 0.100000\n",
            "2021-02-09 16:21:37,452 ----------------------------------------------------------------------------------------------------\n",
            "2021-02-09 16:21:37,453 EPOCH 6 done: loss 1.6885 - lr 0.1000000\n",
            "2021-02-09 16:21:37,663 DEV : loss 1.731760859489441 - score 0.6027\n",
            "2021-02-09 16:21:37,671 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2021-02-09 16:21:38,652 ----------------------------------------------------------------------------------------------------\n",
            "2021-02-09 16:21:39,155 epoch 7 - iter 6/62 - loss 1.63333855 - samples/sec: 386.34 - lr: 0.100000\n",
            "2021-02-09 16:21:39,674 epoch 7 - iter 12/62 - loss 1.58201319 - samples/sec: 370.59 - lr: 0.100000\n",
            "2021-02-09 16:21:40,203 epoch 7 - iter 18/62 - loss 1.61759477 - samples/sec: 364.68 - lr: 0.100000\n",
            "2021-02-09 16:21:40,736 epoch 7 - iter 24/62 - loss 1.60494778 - samples/sec: 361.08 - lr: 0.100000\n",
            "2021-02-09 16:21:41,297 epoch 7 - iter 30/62 - loss 1.57857201 - samples/sec: 343.50 - lr: 0.100000\n",
            "2021-02-09 16:21:41,809 epoch 7 - iter 36/62 - loss 1.60690716 - samples/sec: 375.89 - lr: 0.100000\n",
            "2021-02-09 16:21:42,322 epoch 7 - iter 42/62 - loss 1.65519645 - samples/sec: 375.25 - lr: 0.100000\n",
            "2021-02-09 16:21:42,836 epoch 7 - iter 48/62 - loss 1.63250731 - samples/sec: 375.10 - lr: 0.100000\n",
            "2021-02-09 16:21:43,387 epoch 7 - iter 54/62 - loss 1.61651324 - samples/sec: 349.33 - lr: 0.100000\n",
            "2021-02-09 16:21:43,928 epoch 7 - iter 60/62 - loss 1.62333480 - samples/sec: 356.42 - lr: 0.100000\n",
            "2021-02-09 16:21:44,058 ----------------------------------------------------------------------------------------------------\n",
            "2021-02-09 16:21:44,060 EPOCH 7 done: loss 1.6185 - lr 0.1000000\n",
            "2021-02-09 16:21:44,282 DEV : loss 1.475466012954712 - score 0.6012\n",
            "2021-02-09 16:21:44,290 BAD EPOCHS (no improvement): 1\n",
            "2021-02-09 16:21:44,291 ----------------------------------------------------------------------------------------------------\n",
            "2021-02-09 16:21:44,818 epoch 8 - iter 6/62 - loss 1.48773988 - samples/sec: 365.85 - lr: 0.100000\n",
            "2021-02-09 16:21:45,289 epoch 8 - iter 12/62 - loss 1.46335594 - samples/sec: 409.57 - lr: 0.100000\n",
            "2021-02-09 16:21:45,781 epoch 8 - iter 18/62 - loss 1.44234752 - samples/sec: 391.21 - lr: 0.100000\n",
            "2021-02-09 16:21:46,308 epoch 8 - iter 24/62 - loss 1.51720237 - samples/sec: 365.53 - lr: 0.100000\n",
            "2021-02-09 16:21:46,856 epoch 8 - iter 30/62 - loss 1.52470956 - samples/sec: 351.67 - lr: 0.100000\n",
            "2021-02-09 16:21:47,404 epoch 8 - iter 36/62 - loss 1.50345345 - samples/sec: 351.47 - lr: 0.100000\n",
            "2021-02-09 16:21:47,930 epoch 8 - iter 42/62 - loss 1.48280500 - samples/sec: 365.99 - lr: 0.100000\n",
            "2021-02-09 16:21:48,453 epoch 8 - iter 48/62 - loss 1.49833466 - samples/sec: 368.97 - lr: 0.100000\n",
            "2021-02-09 16:21:48,996 epoch 8 - iter 54/62 - loss 1.49996022 - samples/sec: 354.59 - lr: 0.100000\n",
            "2021-02-09 16:21:49,565 epoch 8 - iter 60/62 - loss 1.46833231 - samples/sec: 338.91 - lr: 0.100000\n",
            "2021-02-09 16:21:49,714 ----------------------------------------------------------------------------------------------------\n",
            "2021-02-09 16:21:49,715 EPOCH 8 done: loss 1.4680 - lr 0.1000000\n",
            "2021-02-09 16:21:49,932 DEV : loss 1.6217914819717407 - score 0.5462\n",
            "2021-02-09 16:21:49,940 BAD EPOCHS (no improvement): 2\n",
            "2021-02-09 16:21:49,941 ----------------------------------------------------------------------------------------------------\n",
            "2021-02-09 16:21:50,474 epoch 9 - iter 6/62 - loss 1.45046393 - samples/sec: 362.53 - lr: 0.100000\n",
            "2021-02-09 16:21:51,007 epoch 9 - iter 12/62 - loss 1.42127867 - samples/sec: 362.31 - lr: 0.100000\n",
            "2021-02-09 16:21:51,505 epoch 9 - iter 18/62 - loss 1.42544189 - samples/sec: 386.88 - lr: 0.100000\n",
            "2021-02-09 16:21:52,035 epoch 9 - iter 24/62 - loss 1.43718036 - samples/sec: 363.39 - lr: 0.100000\n",
            "2021-02-09 16:21:52,534 epoch 9 - iter 30/62 - loss 1.42899894 - samples/sec: 385.88 - lr: 0.100000\n",
            "2021-02-09 16:21:53,052 epoch 9 - iter 36/62 - loss 1.41407406 - samples/sec: 371.58 - lr: 0.100000\n",
            "2021-02-09 16:21:53,572 epoch 9 - iter 42/62 - loss 1.40088659 - samples/sec: 372.88 - lr: 0.100000\n",
            "2021-02-09 16:21:54,059 epoch 9 - iter 48/62 - loss 1.40811325 - samples/sec: 396.10 - lr: 0.100000\n",
            "2021-02-09 16:21:54,590 epoch 9 - iter 54/62 - loss 1.39728696 - samples/sec: 364.91 - lr: 0.100000\n",
            "2021-02-09 16:21:55,133 epoch 9 - iter 60/62 - loss 1.38874779 - samples/sec: 356.37 - lr: 0.100000\n",
            "2021-02-09 16:21:55,300 ----------------------------------------------------------------------------------------------------\n",
            "2021-02-09 16:21:55,302 EPOCH 9 done: loss 1.3918 - lr 0.1000000\n",
            "2021-02-09 16:21:55,532 DEV : loss 1.260438323020935 - score 0.6314\n",
            "2021-02-09 16:21:55,540 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2021-02-09 16:21:56,548 ----------------------------------------------------------------------------------------------------\n",
            "2021-02-09 16:21:57,110 epoch 10 - iter 6/62 - loss 1.52357062 - samples/sec: 343.84 - lr: 0.100000\n",
            "2021-02-09 16:21:57,692 epoch 10 - iter 12/62 - loss 1.41489082 - samples/sec: 330.63 - lr: 0.100000\n",
            "2021-02-09 16:21:58,232 epoch 10 - iter 18/62 - loss 1.40799907 - samples/sec: 357.14 - lr: 0.100000\n",
            "2021-02-09 16:21:58,747 epoch 10 - iter 24/62 - loss 1.36423531 - samples/sec: 373.96 - lr: 0.100000\n",
            "2021-02-09 16:21:59,267 epoch 10 - iter 30/62 - loss 1.35267007 - samples/sec: 369.95 - lr: 0.100000\n",
            "2021-02-09 16:21:59,866 epoch 10 - iter 36/62 - loss 1.32481730 - samples/sec: 321.35 - lr: 0.100000\n",
            "2021-02-09 16:22:00,434 epoch 10 - iter 42/62 - loss 1.29838624 - samples/sec: 339.43 - lr: 0.100000\n",
            "2021-02-09 16:22:00,954 epoch 10 - iter 48/62 - loss 1.29450513 - samples/sec: 370.03 - lr: 0.100000\n",
            "2021-02-09 16:22:01,488 epoch 10 - iter 54/62 - loss 1.29600692 - samples/sec: 360.71 - lr: 0.100000\n",
            "2021-02-09 16:22:01,987 epoch 10 - iter 60/62 - loss 1.29555477 - samples/sec: 385.90 - lr: 0.100000\n",
            "2021-02-09 16:22:02,132 ----------------------------------------------------------------------------------------------------\n",
            "2021-02-09 16:22:02,133 EPOCH 10 done: loss 1.3023 - lr 0.1000000\n",
            "2021-02-09 16:22:02,349 DEV : loss 1.2871795892715454 - score 0.6745\n",
            "2021-02-09 16:22:02,357 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2021-02-09 16:22:03,331 ----------------------------------------------------------------------------------------------------\n",
            "2021-02-09 16:22:03,867 epoch 11 - iter 6/62 - loss 1.30764915 - samples/sec: 359.91 - lr: 0.100000\n",
            "2021-02-09 16:22:04,394 epoch 11 - iter 12/62 - loss 1.24945685 - samples/sec: 365.67 - lr: 0.100000\n",
            "2021-02-09 16:22:04,962 epoch 11 - iter 18/62 - loss 1.24056065 - samples/sec: 339.02 - lr: 0.100000\n",
            "2021-02-09 16:22:05,502 epoch 11 - iter 24/62 - loss 1.29481853 - samples/sec: 356.12 - lr: 0.100000\n",
            "2021-02-09 16:22:06,014 epoch 11 - iter 30/62 - loss 1.28692491 - samples/sec: 377.94 - lr: 0.100000\n",
            "2021-02-09 16:22:06,597 epoch 11 - iter 36/62 - loss 1.30197904 - samples/sec: 330.53 - lr: 0.100000\n",
            "2021-02-09 16:22:07,132 epoch 11 - iter 42/62 - loss 1.30021472 - samples/sec: 359.84 - lr: 0.100000\n",
            "2021-02-09 16:22:07,664 epoch 11 - iter 48/62 - loss 1.28960784 - samples/sec: 362.63 - lr: 0.100000\n",
            "2021-02-09 16:22:08,185 epoch 11 - iter 54/62 - loss 1.28576561 - samples/sec: 369.62 - lr: 0.100000\n",
            "2021-02-09 16:22:08,725 epoch 11 - iter 60/62 - loss 1.30274153 - samples/sec: 357.84 - lr: 0.100000\n",
            "2021-02-09 16:22:08,889 ----------------------------------------------------------------------------------------------------\n",
            "2021-02-09 16:22:08,895 EPOCH 11 done: loss 1.2966 - lr 0.1000000\n",
            "2021-02-09 16:22:09,112 DEV : loss 1.1894261837005615 - score 0.6549\n",
            "2021-02-09 16:22:09,120 BAD EPOCHS (no improvement): 1\n",
            "2021-02-09 16:22:09,122 ----------------------------------------------------------------------------------------------------\n",
            "2021-02-09 16:22:09,638 epoch 12 - iter 6/62 - loss 1.14477919 - samples/sec: 374.20 - lr: 0.100000\n",
            "2021-02-09 16:22:10,174 epoch 12 - iter 12/62 - loss 1.17179892 - samples/sec: 358.80 - lr: 0.100000\n",
            "2021-02-09 16:22:10,721 epoch 12 - iter 18/62 - loss 1.19424682 - samples/sec: 352.32 - lr: 0.100000\n",
            "2021-02-09 16:22:11,242 epoch 12 - iter 24/62 - loss 1.20106575 - samples/sec: 369.65 - lr: 0.100000\n",
            "2021-02-09 16:22:11,742 epoch 12 - iter 30/62 - loss 1.19828815 - samples/sec: 385.32 - lr: 0.100000\n",
            "2021-02-09 16:22:12,252 epoch 12 - iter 36/62 - loss 1.19572676 - samples/sec: 378.05 - lr: 0.100000\n",
            "2021-02-09 16:22:12,816 epoch 12 - iter 42/62 - loss 1.18249927 - samples/sec: 341.79 - lr: 0.100000\n",
            "2021-02-09 16:22:13,352 epoch 12 - iter 48/62 - loss 1.18507020 - samples/sec: 358.99 - lr: 0.100000\n",
            "2021-02-09 16:22:13,862 epoch 12 - iter 54/62 - loss 1.21433788 - samples/sec: 377.75 - lr: 0.100000\n",
            "2021-02-09 16:22:14,372 epoch 12 - iter 60/62 - loss 1.21582924 - samples/sec: 377.32 - lr: 0.100000\n",
            "2021-02-09 16:22:14,520 ----------------------------------------------------------------------------------------------------\n",
            "2021-02-09 16:22:14,521 EPOCH 12 done: loss 1.2219 - lr 0.1000000\n",
            "2021-02-09 16:22:14,741 DEV : loss 1.2720874547958374 - score 0.6588\n",
            "2021-02-09 16:22:14,751 BAD EPOCHS (no improvement): 2\n",
            "2021-02-09 16:22:14,752 ----------------------------------------------------------------------------------------------------\n",
            "2021-02-09 16:22:15,480 epoch 13 - iter 6/62 - loss 1.07532384 - samples/sec: 264.89 - lr: 0.100000\n",
            "2021-02-09 16:22:16,241 epoch 13 - iter 12/62 - loss 1.06737005 - samples/sec: 255.59 - lr: 0.100000\n",
            "2021-02-09 16:22:17,025 epoch 13 - iter 18/62 - loss 1.12665355 - samples/sec: 248.22 - lr: 0.100000\n",
            "2021-02-09 16:22:17,758 epoch 13 - iter 24/62 - loss 1.11849231 - samples/sec: 264.99 - lr: 0.100000\n",
            "2021-02-09 16:22:18,552 epoch 13 - iter 30/62 - loss 1.18222220 - samples/sec: 245.14 - lr: 0.100000\n",
            "2021-02-09 16:22:19,374 epoch 13 - iter 36/62 - loss 1.15548823 - samples/sec: 235.81 - lr: 0.100000\n",
            "2021-02-09 16:22:20,209 epoch 13 - iter 42/62 - loss 1.15502087 - samples/sec: 233.58 - lr: 0.100000\n",
            "2021-02-09 16:22:20,892 epoch 13 - iter 48/62 - loss 1.16809662 - samples/sec: 284.51 - lr: 0.100000\n",
            "2021-02-09 16:22:21,554 epoch 13 - iter 54/62 - loss 1.17539078 - samples/sec: 293.33 - lr: 0.100000\n",
            "2021-02-09 16:22:22,060 epoch 13 - iter 60/62 - loss 1.17214623 - samples/sec: 382.53 - lr: 0.100000\n",
            "2021-02-09 16:22:22,194 ----------------------------------------------------------------------------------------------------\n",
            "2021-02-09 16:22:22,195 EPOCH 13 done: loss 1.1677 - lr 0.1000000\n",
            "2021-02-09 16:22:22,403 DEV : loss 1.198197364807129 - score 0.6667\n",
            "2021-02-09 16:22:22,411 BAD EPOCHS (no improvement): 3\n",
            "2021-02-09 16:22:22,412 ----------------------------------------------------------------------------------------------------\n",
            "2021-02-09 16:22:22,969 epoch 14 - iter 6/62 - loss 1.05365599 - samples/sec: 346.73 - lr: 0.100000\n",
            "2021-02-09 16:22:23,516 epoch 14 - iter 12/62 - loss 1.06607270 - samples/sec: 352.16 - lr: 0.100000\n",
            "2021-02-09 16:22:24,042 epoch 14 - iter 18/62 - loss 1.04821796 - samples/sec: 366.12 - lr: 0.100000\n",
            "2021-02-09 16:22:24,561 epoch 14 - iter 24/62 - loss 1.08410354 - samples/sec: 371.62 - lr: 0.100000\n",
            "2021-02-09 16:22:25,040 epoch 14 - iter 30/62 - loss 1.12521177 - samples/sec: 404.67 - lr: 0.100000\n",
            "2021-02-09 16:22:25,595 epoch 14 - iter 36/62 - loss 1.10881506 - samples/sec: 346.70 - lr: 0.100000\n",
            "2021-02-09 16:22:26,144 epoch 14 - iter 42/62 - loss 1.11724250 - samples/sec: 350.58 - lr: 0.100000\n",
            "2021-02-09 16:22:26,644 epoch 14 - iter 48/62 - loss 1.13691635 - samples/sec: 387.02 - lr: 0.100000\n",
            "2021-02-09 16:22:27,141 epoch 14 - iter 54/62 - loss 1.12417822 - samples/sec: 387.84 - lr: 0.100000\n",
            "2021-02-09 16:22:27,614 epoch 14 - iter 60/62 - loss 1.12373136 - samples/sec: 407.40 - lr: 0.100000\n",
            "2021-02-09 16:22:27,754 ----------------------------------------------------------------------------------------------------\n",
            "2021-02-09 16:22:27,755 EPOCH 14 done: loss 1.1161 - lr 0.1000000\n",
            "2021-02-09 16:22:27,969 DEV : loss 1.2482718229293823 - score 0.6667\n",
            "Epoch    14: reducing learning rate of group 0 to 5.0000e-02.\n",
            "2021-02-09 16:22:27,979 BAD EPOCHS (no improvement): 4\n",
            "2021-02-09 16:22:27,981 ----------------------------------------------------------------------------------------------------\n",
            "2021-02-09 16:22:28,513 epoch 15 - iter 6/62 - loss 1.14691562 - samples/sec: 361.83 - lr: 0.050000\n",
            "2021-02-09 16:22:29,049 epoch 15 - iter 12/62 - loss 1.16271130 - samples/sec: 360.55 - lr: 0.050000\n",
            "2021-02-09 16:22:29,589 epoch 15 - iter 18/62 - loss 1.12237110 - samples/sec: 356.05 - lr: 0.050000\n",
            "2021-02-09 16:22:30,100 epoch 15 - iter 24/62 - loss 1.11425598 - samples/sec: 378.01 - lr: 0.050000\n",
            "2021-02-09 16:22:30,617 epoch 15 - iter 30/62 - loss 1.09107899 - samples/sec: 372.63 - lr: 0.050000\n",
            "2021-02-09 16:22:31,136 epoch 15 - iter 36/62 - loss 1.10042837 - samples/sec: 370.73 - lr: 0.050000\n",
            "2021-02-09 16:22:31,672 epoch 15 - iter 42/62 - loss 1.09206466 - samples/sec: 359.63 - lr: 0.050000\n",
            "2021-02-09 16:22:32,207 epoch 15 - iter 48/62 - loss 1.07316844 - samples/sec: 359.79 - lr: 0.050000\n",
            "2021-02-09 16:22:32,719 epoch 15 - iter 54/62 - loss 1.06471921 - samples/sec: 376.50 - lr: 0.050000\n",
            "2021-02-09 16:22:33,213 epoch 15 - iter 60/62 - loss 1.06643169 - samples/sec: 389.47 - lr: 0.050000\n",
            "2021-02-09 16:22:33,349 ----------------------------------------------------------------------------------------------------\n",
            "2021-02-09 16:22:33,350 EPOCH 15 done: loss 1.0659 - lr 0.0500000\n",
            "2021-02-09 16:22:33,571 DEV : loss 1.0096714496612549 - score 0.7059\n",
            "2021-02-09 16:22:33,579 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2021-02-09 16:22:34,640 ----------------------------------------------------------------------------------------------------\n",
            "2021-02-09 16:22:35,232 epoch 16 - iter 6/62 - loss 0.93289799 - samples/sec: 330.81 - lr: 0.050000\n",
            "2021-02-09 16:22:35,776 epoch 16 - iter 12/62 - loss 0.96079089 - samples/sec: 354.62 - lr: 0.050000\n",
            "2021-02-09 16:22:36,289 epoch 16 - iter 18/62 - loss 0.99790580 - samples/sec: 375.25 - lr: 0.050000\n",
            "2021-02-09 16:22:36,831 epoch 16 - iter 24/62 - loss 0.94963096 - samples/sec: 355.59 - lr: 0.050000\n",
            "2021-02-09 16:22:37,372 epoch 16 - iter 30/62 - loss 0.95210413 - samples/sec: 358.33 - lr: 0.050000\n",
            "2021-02-09 16:22:37,901 epoch 16 - iter 36/62 - loss 0.97272226 - samples/sec: 364.28 - lr: 0.050000\n",
            "2021-02-09 16:22:38,428 epoch 16 - iter 42/62 - loss 0.98063941 - samples/sec: 365.32 - lr: 0.050000\n",
            "2021-02-09 16:22:38,967 epoch 16 - iter 48/62 - loss 0.98688558 - samples/sec: 356.91 - lr: 0.050000\n",
            "2021-02-09 16:22:39,549 epoch 16 - iter 54/62 - loss 0.98487272 - samples/sec: 332.35 - lr: 0.050000\n",
            "2021-02-09 16:22:40,112 epoch 16 - iter 60/62 - loss 0.99180389 - samples/sec: 343.65 - lr: 0.050000\n",
            "2021-02-09 16:22:40,275 ----------------------------------------------------------------------------------------------------\n",
            "2021-02-09 16:22:40,276 EPOCH 16 done: loss 0.9943 - lr 0.0500000\n",
            "2021-02-09 16:22:40,504 DEV : loss 1.0399768352508545 - score 0.7098\n",
            "2021-02-09 16:22:40,510 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2021-02-09 16:22:41,550 ----------------------------------------------------------------------------------------------------\n",
            "2021-02-09 16:22:42,118 epoch 17 - iter 6/62 - loss 1.09879640 - samples/sec: 339.53 - lr: 0.050000\n",
            "2021-02-09 16:22:42,625 epoch 17 - iter 12/62 - loss 1.04698240 - samples/sec: 379.71 - lr: 0.050000\n",
            "2021-02-09 16:22:43,190 epoch 17 - iter 18/62 - loss 0.98961363 - samples/sec: 341.12 - lr: 0.050000\n",
            "2021-02-09 16:22:43,680 epoch 17 - iter 24/62 - loss 1.00052344 - samples/sec: 393.07 - lr: 0.050000\n",
            "2021-02-09 16:22:44,231 epoch 17 - iter 30/62 - loss 1.00643571 - samples/sec: 349.44 - lr: 0.050000\n",
            "2021-02-09 16:22:44,767 epoch 17 - iter 36/62 - loss 1.01471481 - samples/sec: 359.35 - lr: 0.050000\n",
            "2021-02-09 16:22:45,289 epoch 17 - iter 42/62 - loss 0.99803318 - samples/sec: 369.57 - lr: 0.050000\n",
            "2021-02-09 16:22:45,862 epoch 17 - iter 48/62 - loss 1.00052464 - samples/sec: 336.43 - lr: 0.050000\n",
            "2021-02-09 16:22:46,359 epoch 17 - iter 54/62 - loss 1.00696332 - samples/sec: 388.15 - lr: 0.050000\n",
            "2021-02-09 16:22:46,852 epoch 17 - iter 60/62 - loss 1.01201374 - samples/sec: 390.17 - lr: 0.050000\n",
            "2021-02-09 16:22:46,989 ----------------------------------------------------------------------------------------------------\n",
            "2021-02-09 16:22:46,990 EPOCH 17 done: loss 1.0141 - lr 0.0500000\n",
            "2021-02-09 16:22:47,204 DEV : loss 1.0144689083099365 - score 0.6902\n",
            "2021-02-09 16:22:47,209 BAD EPOCHS (no improvement): 1\n",
            "2021-02-09 16:22:47,212 ----------------------------------------------------------------------------------------------------\n",
            "2021-02-09 16:22:47,728 epoch 18 - iter 6/62 - loss 1.03632129 - samples/sec: 373.12 - lr: 0.050000\n",
            "2021-02-09 16:22:48,293 epoch 18 - iter 12/62 - loss 1.00514870 - samples/sec: 343.63 - lr: 0.050000\n",
            "2021-02-09 16:22:48,816 epoch 18 - iter 18/62 - loss 1.01106942 - samples/sec: 367.92 - lr: 0.050000\n",
            "2021-02-09 16:22:49,341 epoch 18 - iter 24/62 - loss 1.02452123 - samples/sec: 367.26 - lr: 0.050000\n",
            "2021-02-09 16:22:49,862 epoch 18 - iter 30/62 - loss 1.00423632 - samples/sec: 370.76 - lr: 0.050000\n",
            "2021-02-09 16:22:50,376 epoch 18 - iter 36/62 - loss 1.01601396 - samples/sec: 375.00 - lr: 0.050000\n",
            "2021-02-09 16:22:50,886 epoch 18 - iter 42/62 - loss 1.00617369 - samples/sec: 377.54 - lr: 0.050000\n",
            "2021-02-09 16:22:51,417 epoch 18 - iter 48/62 - loss 0.99030736 - samples/sec: 363.43 - lr: 0.050000\n",
            "2021-02-09 16:22:51,994 epoch 18 - iter 54/62 - loss 0.99812055 - samples/sec: 334.13 - lr: 0.050000\n",
            "2021-02-09 16:22:52,487 epoch 18 - iter 60/62 - loss 0.99099064 - samples/sec: 391.06 - lr: 0.050000\n",
            "2021-02-09 16:22:52,628 ----------------------------------------------------------------------------------------------------\n",
            "2021-02-09 16:22:52,630 EPOCH 18 done: loss 0.9988 - lr 0.0500000\n",
            "2021-02-09 16:22:52,843 DEV : loss 1.1750390529632568 - score 0.6549\n",
            "2021-02-09 16:22:52,851 BAD EPOCHS (no improvement): 2\n",
            "2021-02-09 16:22:52,853 ----------------------------------------------------------------------------------------------------\n",
            "2021-02-09 16:22:53,355 epoch 19 - iter 6/62 - loss 1.02942906 - samples/sec: 384.46 - lr: 0.050000\n",
            "2021-02-09 16:22:53,906 epoch 19 - iter 12/62 - loss 0.93519984 - samples/sec: 349.51 - lr: 0.050000\n",
            "2021-02-09 16:22:54,407 epoch 19 - iter 18/62 - loss 0.98329491 - samples/sec: 384.42 - lr: 0.050000\n",
            "2021-02-09 16:22:54,931 epoch 19 - iter 24/62 - loss 0.99379317 - samples/sec: 367.70 - lr: 0.050000\n",
            "2021-02-09 16:22:55,448 epoch 19 - iter 30/62 - loss 0.97770625 - samples/sec: 372.47 - lr: 0.050000\n",
            "2021-02-09 16:22:55,988 epoch 19 - iter 36/62 - loss 0.95471958 - samples/sec: 356.92 - lr: 0.050000\n",
            "2021-02-09 16:22:56,480 epoch 19 - iter 42/62 - loss 0.95329399 - samples/sec: 391.99 - lr: 0.050000\n",
            "2021-02-09 16:22:56,996 epoch 19 - iter 48/62 - loss 0.94582481 - samples/sec: 373.26 - lr: 0.050000\n",
            "2021-02-09 16:22:57,529 epoch 19 - iter 54/62 - loss 0.94968269 - samples/sec: 360.89 - lr: 0.050000\n",
            "2021-02-09 16:22:58,066 epoch 19 - iter 60/62 - loss 0.95281128 - samples/sec: 361.83 - lr: 0.050000\n",
            "2021-02-09 16:22:58,217 ----------------------------------------------------------------------------------------------------\n",
            "2021-02-09 16:22:58,218 EPOCH 19 done: loss 0.9534 - lr 0.0500000\n",
            "2021-02-09 16:22:58,431 DEV : loss 1.0047805309295654 - score 0.7137\n",
            "2021-02-09 16:22:58,439 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2021-02-09 16:22:59,459 ----------------------------------------------------------------------------------------------------\n",
            "2021-02-09 16:23:00,029 epoch 20 - iter 6/62 - loss 0.90976047 - samples/sec: 346.48 - lr: 0.050000\n",
            "2021-02-09 16:23:00,575 epoch 20 - iter 12/62 - loss 0.89615626 - samples/sec: 356.50 - lr: 0.050000\n",
            "2021-02-09 16:23:01,142 epoch 20 - iter 18/62 - loss 0.90207269 - samples/sec: 339.79 - lr: 0.050000\n",
            "2021-02-09 16:23:01,668 epoch 20 - iter 24/62 - loss 0.92222869 - samples/sec: 366.37 - lr: 0.050000\n",
            "2021-02-09 16:23:02,181 epoch 20 - iter 30/62 - loss 0.91905120 - samples/sec: 374.70 - lr: 0.050000\n",
            "2021-02-09 16:23:02,668 epoch 20 - iter 36/62 - loss 0.90504155 - samples/sec: 396.40 - lr: 0.050000\n",
            "2021-02-09 16:23:03,197 epoch 20 - iter 42/62 - loss 0.90205150 - samples/sec: 364.27 - lr: 0.050000\n",
            "2021-02-09 16:23:03,703 epoch 20 - iter 48/62 - loss 0.90723671 - samples/sec: 380.38 - lr: 0.050000\n",
            "2021-02-09 16:23:04,243 epoch 20 - iter 54/62 - loss 0.91276192 - samples/sec: 358.59 - lr: 0.050000\n",
            "2021-02-09 16:23:04,783 epoch 20 - iter 60/62 - loss 0.91889961 - samples/sec: 356.93 - lr: 0.050000\n",
            "2021-02-09 16:23:04,915 ----------------------------------------------------------------------------------------------------\n",
            "2021-02-09 16:23:04,916 EPOCH 20 done: loss 0.9242 - lr 0.0500000\n",
            "2021-02-09 16:23:05,134 DEV : loss 1.0047662258148193 - score 0.7059\n",
            "2021-02-09 16:23:05,142 BAD EPOCHS (no improvement): 1\n",
            "2021-02-09 16:23:06,160 ----------------------------------------------------------------------------------------------------\n",
            "2021-02-09 16:23:06,165 Testing using best model ...\n",
            "2021-02-09 16:23:06,173 loading file models/baseline-charembeddings/best-model.pt\n",
            "2021-02-09 16:23:07,600 0.6933\t0.6944\t0.6938\n",
            "2021-02-09 16:23:07,603 \n",
            "Results:\n",
            "- F1-score (micro) 0.6938\n",
            "- F1-score (macro) 0.6114\n",
            "\n",
            "By class:\n",
            "BOOK       tp: 20 - fp: 45 - fn: 31 - precision: 0.3077 - recall: 0.3922 - f1-score: 0.3448\n",
            "COMPOSER   tp: 34 - fp: 4 - fn: 25 - precision: 0.8947 - recall: 0.5763 - f1-score: 0.7010\n",
            "FILM       tp: 24 - fp: 2 - fn: 51 - precision: 0.9231 - recall: 0.3200 - f1-score: 0.4752\n",
            "SINGER     tp: 217 - fp: 123 - fn: 32 - precision: 0.6382 - recall: 0.8715 - f1-score: 0.7368\n",
            "SONG       tp: 139 - fp: 18 - fn: 52 - precision: 0.8854 - recall: 0.7277 - f1-score: 0.7989\n",
            "2021-02-09 16:23:07,604 ----------------------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFhYESj2eMT7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
